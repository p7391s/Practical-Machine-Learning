---
title: "Prediction Using Writeup Lifting Exercises Dataset"
author: "ps7391"
date: '25 january 2019'
output: 
        html_document:
                keep_md: yes
        md_document:
                variant: markdown_github
        pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

### Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

### Executive summary

Based on a dataset provide by HAR http://groupware.les.inf.puc-rio.br/har we will try to train a predictive model to perdict what exercise was performed using a dataset with 159 features.

We'll take the following steps:

* Process the data, for use to this project
* Explore the data, especially focussing on the two parameters we are interested in
* Model selection, where we try different models to help us answere our questions
* Model examination, to see wether our best model holds up to our standards
* Conclusion where we answere the questions based on the data
* Predicting the classification of the model on test set.

### Libraries

```{r libraries}
library(lattice)
library(ggplot2)
library(plyr)
library(randomForest)
library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
library(corrplot)
library(RColorBrewer)
```

### Environment Preparation

First prepare the environment:
```{r}
## work dir
path <- getwd()
```

### Dataset Overview

The training data for this project are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv 

The test data are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come frome http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har. 

My special thanks to the mentioned authors for being so generous in allowing their data to be used for this kind of assignment.

### Loading Data

The next step is loading the dataset from the URL provided above.

```{r}
## url training data file
url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"

file_train <- "pml-training.csv"

if(!file.exists(file_train)){
	print("train_file_downloaded")
	download.file(url_train, file_train, method = "curl")
}

## url test data file
url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

file_test <- "pml-testing.csv"

if(!file.exists(file_test)){
	print("test_file_downloaded")
	download.file(url_test, file_test, method = "curl")
}
```

### Preprocessing

### Reading data and evaluating their structure

```{r}
## processing
training <- read.csv("pml-training.csv")
testing <-read.csv("pml-testing.csv")

## exploratory data analyses
dim(training)  
dim(testing) 
#head(training) #if necessary

#str(training) #if necessary

#summary(training) #if necessary
```

### Creating Partition

```{r}
## create a partition with the training dataset
in_train <- createDataPartition(training$classe, p = 0.75, list = FALSE)
train_set <- training[in_train, ]
test_set <- training[-in_train, ]
#dim(train_set) #if necessary

#dim(test_set) #if necessary
```

### Cleaning and Data Preparation

```{r}
## remove variables with NA variance
nvz <- nearZeroVar(train_set)
train_set <- train_set[, -nvz]
test_set <- test_set[, -nvz]


## remove variables mostly NA
all_na <- sapply(train_set, function(x) mean(is.na(x))) > 0.95
train_set <- train_set[, all_na==FALSE]
test_set <- test_set[, all_na==FALSE]

## remove identification only variables
train_set <- train_set[, -(1:5)]
test_set <- test_set[, -(1:5)]
dim(train_set)
dim(test_set)

```

### Exploratory Data Analyses

Determine which variables are highly correlated with the class variable.

```{r, fig.align= 'center', fig.cap='Figure 1. Correlation matrix', out.width='100%'}
## correlation analysis
library(ggcorrplot)
cor_matrix <- cor(train_set[, -54])
ggcorrplot(cor_matrix, type = "lower",
	colors = c("purple", "white", "blue"))
```

Even the best correlations with classe are hardly above 0.3. Let's check visually if there is indeed hard to use these 2 as possible simple linear predictors.

```{r, fig.align= 'center', fig.cap='Figure 2. The best correlations with classe'}
library(Rmisc)
library(ggplot2)
library(RColorBrewer)

g1 <- ggplot(train_set, aes(classe, pitch_forearm)) +
	geom_boxplot(aes(fill = classe), col = "darkred", fill = "red") +
	scale_fill_brewer(palette="RColorBrewer") +
 	theme_dark()

g2 <- ggplot(train_set, aes(classe, magnet_arm_x)) +
	geom_boxplot(aes(fill = classe), col = "blue", fill = "lightblue") +
	scale_fill_brewer(palette="Blues") +
	theme_dark()

multiplot(g1, g2, cols = 2)
```

### Prediction Models Building

Tree methods will be applied to model the regressions (in the train dataset) and the best one (with higher accuracy when applied to the test dataset) will be used for the quiz predictions. The methods are: Random Forests (randomForest), Decision Tree (rpart) and Support Machine Vector (svm).

#### Random Forest

```{r, random_forest, fig.align= 'center', fig.cap='Figure 3. Random Forest'}
## method random forest
set.seed(1)
controlRF <- trainControl(method = "cv", number = 3, verboseIter = FALSE)
modFitRandForest <- train(classe ~ ., data = train_set, method = "rf",
	trControl = controlRF)
modFitRandForest$finalModel

## prediction on test data
predictRandForest <- predict(modFitRandForest, newdata = test_set)
confMatRandForest <- confusionMatrix(predictRandForest, test_set$classe)
confMatRandForest

## plot matrix results
plot(confMatRandForest$table, col = confMatRandForest$byClass,
	color = c("red", "green", "yellow", "violet", "blue"),
	main = paste("Random Forest - Accuracy = ",
		round(confMatRandForest$overall['Accuracy'], 4)))
```

The accuracy result is 99%, enough to get the prediction of the 20 values.

#### Decision Trees

```{r, decision trees, fig.align= 'center', fig.cap='Figure 4. Decision Trees'}
## method decision trees
set.seed(1)
mod_fit_dec_tree <- train(classe ~., data = train_set, method = "rpart")
fancyRpartPlot(mod_fit_dec_tree$finalModel)

pv <- predict(mod_fit_dec_tree, newdata = test_set)
cm_ct <- confusionMatrix(pv, test_set$classe)
cm_ct$cm_ct$overall['Accuracy']

cm_ct
```

#### Support Vector Machine

```{r, support vector machine, fig.align= 'center', fig.cap='Figure 5. Support Vector Machine'}
## support vector machine

library(dplyr)
library(e1071)

mod_train_svm <- svm(classe ~., data = train_set)
mod_predict_svm <- predict(mod_train_svm, test_set)
cm_svm <- confusionMatrix(mod_predict_svm, test_set$classe)
cm_svm

plot(cm_svm$table, col = cm_svm$byClass,
	color = c("red", "green", "yellow", "violet", "blue"),
	main = paste("Support Vector Machine - Accuracy = ",
		round(cm_svm$overall['Accuracy'], 4)))
```

The accuracy result is 94 %.

### Applying the Selected Model to the Test Data

The best accuracy of the 3 regression modeling methods above is Random Forest - 99%.

In that case, the Random Forest model will be applied to predict the 20 quiz results (testing dataset) as shown below.

```{r}
predict_test <- predict(modFitRandForest, newdata = testing)
predict_test
```